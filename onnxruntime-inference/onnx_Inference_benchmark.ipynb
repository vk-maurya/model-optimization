{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qmPERnxpNPH",
        "outputId": "207b7cd1-c788-4f7a-bfa4-2bed1a461bbe"
      },
      "outputs": [],
      "source": [
        "# %pip install onnxruntime-gpu onnx transformers psutil pandas py-cpuinfo py3nvml coloredlogs wget netron sympy datasets torchmetrics ipython-autotime accelerate protobuf watermark -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EWNcRzupxN1",
        "outputId": "634fe285-7441-4803-bbcd-eae039cbc7a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 101 µs (started: 2024-05-18 17:50:14 +05:30)\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import os\n",
        "import os.path as op\n",
        "import time\n",
        "\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from watermark import watermark\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "torch.manual_seed(123)\n",
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pKz7kp_qFs9",
        "outputId": "87219311-875f-4258-b37e-201c1b003b4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Last updated: 2024-05-18T17:50:18.255726+05:30\n",
            "\n",
            "Python implementation: CPython\n",
            "Python version       : 3.12.3\n",
            "IPython version      : 8.24.0\n",
            "\n",
            "Compiler    : GCC 11.2.0\n",
            "OS          : Linux\n",
            "Release     : 5.15.146.1-microsoft-standard-WSL2\n",
            "Machine     : x86_64\n",
            "Processor   : x86_64\n",
            "CPU cores   : 20\n",
            "Architecture: 64bit\n",
            "\n",
            "onnxruntime : 1.17.1\n",
            "onnx        : 1.16.0\n",
            "transformers: 4.41.0\n",
            "accelerate  : 0.30.1\n",
            "datasets    : 2.19.1\n",
            "torchmetrics: 1.4.0.post0\n",
            "pandas      : 2.2.2\n",
            "\n",
            "time: 1.31 s (started: 2024-05-18 17:50:18 +05:30)\n"
          ]
        }
      ],
      "source": [
        "print(watermark())\n",
        "print(watermark(packages=\"onnxruntime,onnx,transformers,accelerate,datasets,torchmetrics,pandas\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwaTVpDdpt5G",
        "outputId": "61a60cf3-986f-4d6f-e574-7225429ca031"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch CUDA available? True\n",
            "time: 30.9 ms (started: 2024-05-18 17:50:22 +05:30)\n"
          ]
        }
      ],
      "source": [
        "print(\"Torch CUDA available?\", torch.cuda.is_available())\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "c74f684c918d49f18e12d876648eeece",
            "481ab94682624ab981a95717c9009f28",
            "4db34daeff2540869846d682a46c0f0a",
            "b257188ca29445b6a9ff90e73378c846",
            "3c6aee6683ad4fc9bf5ae17b23746024",
            "e12f95259ed94fed822db2fb480b7393",
            "250374317d9a4cb08459bc2c92f35f30",
            "803d395d72a84c8985f9c6bc308f71d1",
            "46567763914c4844a5676969b01442e2",
            "8ab7ab272cd3433287d5d1e8a038dfde",
            "c579999eea9d4d4f8b5b5503207fab62",
            "afe8f97f4eb04b6bae90f3f39d82cd8c",
            "7bfef0686b654a7daf322675dfe88954",
            "4fcc587f74ff4171b36f0e809d35cb8c",
            "ca879a396d1a4d5e869783a3b7ba8099",
            "45fb4384dd914cab9ae68a322134ee09",
            "df0b98f6de844596802f7b76d6217fc7",
            "d9adaa1cd37649f5a0faa40112143926",
            "acfb1ea1b0e04cb39d4ba8d5309755b8",
            "aca69b5875e644ae9669ccc2d743a889",
            "2460da69bf824304915dc26ae8e0d8ab",
            "638d726d1cc6439fb80e447a6de683e5"
          ]
        },
        "id": "j9GRvnMjqd2m",
        "outputId": "469721e9-0c32-421d-dc8c-d2e6956e6db1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating CSV from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 34.39ba/s]\n",
            "Creating CSV from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 26.38ba/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "6461083"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 9.77 s (started: 2024-05-18 17:17:52 +05:30)\n"
          ]
        }
      ],
      "source": [
        "data = load_dataset('imdb')\n",
        "data['train'] = data['train'].shuffle(seed=42)\n",
        "data['test'] = data['test'].shuffle(seed=42)\n",
        "\n",
        "# Select only 5000 records from the train and test datasets\n",
        "data['train'] = data['train'].select(range(5000))\n",
        "data['test'] = data['test'].select(range(5000))\n",
        "# saving into csv\n",
        "data['train'].to_csv(\"train.csv\",index=False)\n",
        "data['test'].to_csv(\"validation.csv\",index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWlyzZvkqnZs",
        "outputId": "aba24a2b-baf5-4349-fd9d-17361be5c367"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 143 ms (started: 2024-05-18 17:51:25 +05:30)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import Dataset\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"validation.csv\")\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDrmkQrGq0KU",
        "outputId": "ba78be97-19c1-46d4-b935-400742fe4704"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\", num_labels=2\n",
        ")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3yfkDplrhOT",
        "outputId": "fae00a6c-57dc-4836-941e-51b1b458a653"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 262 µs (started: 2024-05-18 17:51:46 +05:30)\n"
          ]
        }
      ],
      "source": [
        "max_len = 512\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loader "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNsrJhIvq_Cl",
        "outputId": "50da97b7-6d77-4b98-adef-917b9630894a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 9.86 s (started: 2024-05-18 17:18:44 +05:30)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Define the IMDBDataset class\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_len, device):\n",
        "        self.device = device\n",
        "        texts = df.iloc[:, 0].tolist()\n",
        "        labels = df.iloc[:, 1].tolist()\n",
        "\n",
        "        input_ids = []\n",
        "        attention_masks = []\n",
        "\n",
        "        for text in texts:\n",
        "            encoding = tokenizer.encode_plus(\n",
        "                text,\n",
        "                add_special_tokens=True,\n",
        "                max_length=max_len,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_attention_mask=True,\n",
        "                return_tensors='pt'  # Get PyTorch tensors\n",
        "            )\n",
        "            input_ids.append(encoding['input_ids'])\n",
        "            attention_masks.append(encoding['attention_mask'])\n",
        "        # Convert the lists into tensors and move them to the device\n",
        "        # so that we don't have to do this on for loop during inference\n",
        "        self.input_ids = torch.cat(input_ids).to(device)\n",
        "        self.attention_masks = torch.cat(attention_masks).to(device)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.attention_masks[idx],\n",
        "            'labels': self.labels[idx]\n",
        "        }\n",
        "\n",
        "\n",
        "train= pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"validation.csv\")\n",
        "\n",
        "# Check if CUDA is available and set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Create the dataset and data loader\n",
        "train_dataset = IMDBDataset(train, tokenizer, max_len=max_len, device=device)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Create the dataset and data loader\n",
        "test_dataset = IMDBDataset(test, tokenizer, max_len=max_len, device=device)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kVwuIShrtFs",
        "outputId": "292d40d1-8340-4a0e-e83e-f5adc406ce60"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 15.3 ms (started: 2024-05-18 17:19:08 +05:30)\n"
          ]
        }
      ],
      "source": [
        "len(train_dataset[0]['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "od7Srmnbq0d8",
        "outputId": "b2d3a865-6d49-4afa-903c-6146ab791bde"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [01:38<00:00, 50.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch cuda Inference time = 19.55 ms\n",
            "time: 1min 38s (started: 2024-05-18 17:19:25 +05:30)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "# Measure the latency. It is not accurate using Jupyter Notebook, it is recommended to use standalone python script.\n",
        "latency = []\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(train_dataset):\n",
        "        # both are already in the device\n",
        "        inputs = {\n",
        "            'input_ids':      batch['input_ids'],\n",
        "            'attention_mask': batch['attention_mask']\n",
        "        }\n",
        "        start = time.time()\n",
        "        outputs = model(**inputs)\n",
        "        latency.append(time.time() - start)\n",
        "print(\"PyTorch {} Inference time = {} ms\".format(device.type, format(sum(latency) * 1000 / len(latency), '.2f')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Onnx Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bhp2CEhNs3wg",
        "outputId": "50ee0204-ab11-41cd-9ccd-35295a36d577"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 240 µs (started: 2024-05-18 17:53:31 +05:30)\n"
          ]
        }
      ],
      "source": [
        "# !pip install onnxruntime-gpu onnx -q\n",
        "# !pip install onnxruntime-gpu --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPL5i4-8ttHx",
        "outputId": "4a26252c-8aa1-4922-bc97-6aa6238834a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 512])\n",
            "time: 16.1 ms (started: 2024-05-18 17:54:40 +05:30)\n"
          ]
        }
      ],
      "source": [
        "# data type should match to input datatype\n",
        "# input_ids: torch.int64\n",
        "# attention_mask: torch.int64\n",
        "dummy_input = {\n",
        "    'input_ids': torch.randint(0, 1000, (1, max_len)),              \n",
        "    'attention_mask': torch.ones((1, max_len),dtype=torch.int64)    \n",
        "}\n",
        "print(dummy_input['input_ids'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzVdYbQ-ucCG",
        "outputId": "987e693f-be18-4262-d5f2-93b9a0c8b309"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 2.5 ms (started: 2024-05-18 17:56:32 +05:30)\n"
          ]
        }
      ],
      "source": [
        "# putting model and dummy data into same device\n",
        "dummy_input['input_ids'] = dummy_input['input_ids'].to(device)\n",
        "dummy_input['attention_mask'] =  dummy_input['attention_mask'].to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model device: cuda:0\n",
            "Dummy input device: cuda:0\n",
            "time: 660 µs (started: 2024-05-18 17:56:34 +05:30)\n"
          ]
        }
      ],
      "source": [
        "# checking model and dummy data in same device\n",
        "print(f\"Model device: {model.device}\")\n",
        "print(f\"Dummy input device: {dummy_input['input_ids'].device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0KUGAtItZiQ",
        "outputId": "2980a7c8-fafb-491c-cd52-3de237de284e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/miniconda3/envs/hf/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:231: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
            "  mask, torch.tensor(torch.finfo(scores.dtype).min)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model has been exported to ONNX format.\n",
            "time: 2.16 s (started: 2024-05-18 16:41:54 +05:30)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import onnx\n",
        "\n",
        "# Export the model\n",
        "torch.onnx.export(\n",
        "    model,                            # Model being run\n",
        "    (dummy_input['input_ids'], dummy_input['attention_mask']),  # Model input (or a tuple for multiple inputs)\n",
        "    \"model.onnx\",                     # Where to save the model (can be a file or file-like object)\n",
        "    export_params=True,               # Store the trained parameter weights inside the model file\n",
        "    opset_version=11,                 # ONNX version to export the model to\n",
        "    input_names=['input_ids', 'attention_mask'],  # Input names for the model\n",
        "    output_names=['output'],          # Output names for the model\n",
        "    dynamic_axes={\n",
        "        'input_ids': {0: 'batch_size', 1: 'sequence'},\n",
        "        'attention_mask': {0: 'batch_size', 1: 'sequence'}\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"Model has been exported to ONNX format.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data loader without putting into Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 3.86 s (started: 2024-05-18 17:25:57 +05:30)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "max_len = 512\n",
        "batch_size = 32\n",
        "\n",
        "# Define the IMDBDataset class\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_len, device):\n",
        "        self.device = device\n",
        "        texts = df.iloc[:, 0].tolist()\n",
        "        labels = df.iloc[:, 1].tolist()\n",
        "\n",
        "        input_ids = []\n",
        "        attention_masks = []\n",
        "\n",
        "        for text in texts:\n",
        "            encoding = tokenizer.encode_plus(\n",
        "                text,\n",
        "                add_special_tokens=True,\n",
        "                max_length=max_len,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_attention_mask=True,\n",
        "                return_tensors='pt'  # Get PyTorch tensors\n",
        "            )\n",
        "            input_ids.append(encoding['input_ids'])\n",
        "            attention_masks.append(encoding['attention_mask'])\n",
        "\n",
        "        # Convert the lists into tensors \n",
        "        # we are not moving to cuda becuase onnx runtime requires input in cpu\n",
        "        self.input_ids = torch.cat(input_ids)\n",
        "        self.attention_masks = torch.cat(attention_masks)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.attention_masks[idx],\n",
        "            'labels': self.labels[idx]\n",
        "        }\n",
        "\n",
        "\n",
        "train= pd.read_csv(\"train.csv\")\n",
        "# test = pd.read_csv(\"validation.csv\")\n",
        "\n",
        "# Check if CUDA is available and set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Create the dataset and data loader\n",
        "train_dataset = IMDBDataset(train, tokenizer, max_len=max_len, device=device)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Create the dataset and data loader\n",
        "# test_dataset = IMDBDataset(test, tokenizer, max_len=max_len, device=device)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "import os\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'GPU'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 2.26 ms (started: 2024-05-18 18:00:06 +05:30)\n"
          ]
        }
      ],
      "source": [
        "import onnxruntime \n",
        "onnxruntime.get_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6ScQwslCxrfL"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[0;93m2024-05-18 17:21:53.745879482 [W:onnxruntime:, inference_session.cc:1914 Initialize] Serializing optimized model with Graph Optimization level greater than ORT_ENABLE_EXTENDED and the NchwcTransformer enabled. The generated model may contain hardware specific optimizations, and should only be used in the same environment the model was optimized in.\u001b[m\n",
            "100%|██████████| 5000/5000 [01:07<00:00, 74.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ONNX Runtime GPU Inference time = 13.16 ms\n",
            "time: 1min 9s (started: 2024-05-18 17:21:51 +05:30)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import psutil\n",
        "import onnxruntime\n",
        "import numpy\n",
        "\n",
        "from tqdm import tqdm\n",
        "assert 'CUDAExecutionProvider' in onnxruntime.get_available_providers()\n",
        "device_name = 'gpu'\n",
        "\n",
        "sess_options = onnxruntime.SessionOptions()\n",
        "output_dir =\"optimized\"\n",
        "# Optional: store the optimized graph and view it using Netron to verify that model is fully optimized.\n",
        "# Note that this will increase session creation time so enable it for debugging only.\n",
        "sess_options.optimized_model_filepath = os.path.join(output_dir, \"optimized_model_{}.onnx\".format(device_name))\n",
        "\n",
        "# Please change the value according to best setting in Performance Test Tool result.\n",
        "sess_options.intra_op_num_threads=psutil.cpu_count(logical=True)\n",
        "export_model_path  =\"model.onnx\"\n",
        "session = onnxruntime.InferenceSession(export_model_path, sess_options, providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\n",
        "\n",
        "latency = []\n",
        "for batch in tqdm(train_dataset):\n",
        "    # Ensure the inputs are 2D (batch_size, sequence_length)\n",
        "    input_ids = batch['input_ids'].reshape(1, max_len).numpy()\n",
        "    attention_mask = batch['attention_mask'].reshape(1, max_len).numpy()\n",
        "\n",
        "\n",
        "    inputs = {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_mask\n",
        "    }\n",
        "\n",
        "    start = time.time()\n",
        "    outputs = session.run(None, inputs)\n",
        "    latency.append(time.time() - start)\n",
        "\n",
        "print(\"ONNX Runtime GPU Inference time = {} ms\".format(format(sum(latency) * 1000 / len(latency), '.2f')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [01:06<00:00, 75.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ONNX Runtime GPU Inference time = 13.09 ms\n",
            "time: 1min 8s (started: 2024-05-18 17:28:39 +05:30)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import psutil\n",
        "import onnxruntime\n",
        "import numpy\n",
        "\n",
        "from tqdm import tqdm\n",
        "assert 'CUDAExecutionProvider' in onnxruntime.get_available_providers()\n",
        "device_name = 'gpu'\n",
        "\n",
        "# sess_options = onnxruntime.SessionOptions()\n",
        "# output_dir =\"optimized\"\n",
        "# # Optional: store the optimized graph and view it using Netron to verify that model is fully optimized.\n",
        "# # Note that this will increase session creation time so enable it for debugging only.\n",
        "# sess_options.optimized_model_filepath = os.path.join(output_dir, \"optimized_model_{}.onnx\".format(device_name))\n",
        "\n",
        "# # Please change the value according to best setting in Performance Test Tool result.\n",
        "# sess_options.intra_op_num_threads=psutil.cpu_count(logical=True)\n",
        "# export_model_path  =\"model.onnx\"\n",
        "optimized_model = \"optimized/optimized_model_gpu.onnx\"\n",
        "session = onnxruntime.InferenceSession(optimized_model, providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\n",
        "\n",
        "latency = []\n",
        "for batch in tqdm(train_dataset):\n",
        "    # Ensure the inputs are 2D (batch_size, sequence_length)\n",
        "    input_ids = batch['input_ids'].reshape(1, max_len).numpy()\n",
        "    attention_mask = batch['attention_mask'].reshape(1, max_len).numpy()\n",
        "\n",
        "\n",
        "    inputs = {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_mask\n",
        "    }\n",
        "\n",
        "    start = time.time()\n",
        "    outputs = session.run(None, inputs)\n",
        "    latency.append(time.time() - start)\n",
        "\n",
        "print(\"ONNX Runtime GPU Inference time = {} ms\".format(format(sum(latency) * 1000 / len(latency), '.2f')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pytorch infernece time is 1.38 s\n",
        "# onnx inference time is 1.09 s\n",
        "# onnx is faster than pytorch by 0.29 s\n",
        "# onnx is faster than pytorch by 21.01% = (1.38-1.09)/1.38*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "EfeIOLMVvJyO",
        "outputId": "7f8898c9-fd7d-4808-b610-8d267c97e23a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 39/1000 [00:37<15:13,  1.05it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-7e64a3d52a56>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mlatency\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0moutput_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_meta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPFail\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_fallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 37.6 s (started: 2024-05-18 08:16:04 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# import onnxruntime\n",
        "# import numpy as np\n",
        "# import time\n",
        "# from tqdm import tqdm\n",
        "# sess_options.intra_op_num_threads=psutil.cpu_count(logical=True)\n",
        "# # Load the ONNX model with GPU support\n",
        "# onnx_model_path = \"model.onnx\"\n",
        "# providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
        "# session = onnxruntime.InferenceSession(onnx_model_path, providers=providers)\n",
        "\n",
        "# # Prepare inputs and measure latency\n",
        "# latency = []\n",
        "\n",
        "# # Assuming 'train_dataset' provides batched input tensors\n",
        "# for batch in tqdm(train_dataset):\n",
        "#     # Ensure the inputs are 2D (batch_size, sequence_length)\n",
        "#     input_ids = batch['input_ids'].cpu().reshape(1, max_len).numpy()\n",
        "#     attention_mask = batch['attention_mask'].cpu().reshape(1, max_len).numpy()\n",
        "\n",
        "\n",
        "#     inputs = {\n",
        "#         'input_ids': input_ids,\n",
        "#         'attention_mask': attention_mask\n",
        "#     }\n",
        "\n",
        "#     start = time.time()\n",
        "#     outputs = session.run(None, inputs)\n",
        "#     latency.append(time.time() - start)\n",
        "\n",
        "# print(\"ONNX Runtime GPU Inference time = {} ms\".format(format(sum(latency) * 1000 / len(latency), '.2f')))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4lBQfyLz4rx"
      },
      "outputs": [],
      "source": [
        "import psutil\n",
        "import onnxruntime\n",
        "import numpy\n",
        "\n",
        "assert 'CUDAExecutionProvider' in onnxruntime.get_available_providers()\n",
        "device_name = 'gpu'\n",
        "\n",
        "sess_options = onnxruntime.SessionOptions()\n",
        "\n",
        "# Optional: store the optimized graph and view it using Netron to verify that model is fully optimized.\n",
        "# Note that this will increase session creation time so enable it for debugging only.\n",
        "output_dir = \"output\"\n",
        "sess_options.optimized_model_filepath = os.path.join(output_dir, \"optimized_model_{}.onnx\".format(device_name))\n",
        "\n",
        "# Please change the value according to best setting in Performance Test Tool result.\n",
        "sess_options.intra_op_num_threads=psutil.cpu_count(logical=True)\n",
        "\n",
        "session = onnxruntime.InferenceSession(export_model_path, sess_options, providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\n",
        "\n",
        "latency = []\n",
        "for i in range(total_samples):\n",
        "    data = dataset[i]\n",
        "    ort_inputs = {\n",
        "        'input_ids':  data[0].cpu().reshape(1, max_seq_length).numpy(),\n",
        "        'input_mask': data[1].cpu().reshape(1, max_seq_length).numpy(),\n",
        "        'segment_ids': data[2].cpu().reshape(1, max_seq_length).numpy()\n",
        "    }\n",
        "    start = time.time()\n",
        "    ort_outputs = session.run(None, ort_inputs)\n",
        "    latency.append(time.time() - start)\n",
        "\n",
        "print(\"OnnxRuntime {} Inference time = {} ms\".format(device_name, format(sum(latency) * 1000 / len(latency), '.2f')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_uRZwC4wicM",
        "outputId": "9d7a9f92-7cdf-4d7e-9b13-572f06d1e9b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 8.44 ms (started: 2024-05-18 08:14:28 +00:00)\n"
          ]
        }
      ],
      "source": [
        "dummy_input = {\n",
        "    'input_ids': torch.randint(0, 1000, (1, max_len)),  # Example input, adjust as necessary\n",
        "    'attention_mask': torch.ones((1, max_len)).int()          # Example input, adjust as necessary\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50PyfufbwjW8",
        "outputId": "5cc75877-b172-40d4-d350-47cec795ffe0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1]], dtype=int32)"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 25.3 ms (started: 2024-05-18 08:14:31 +00:00)\n"
          ]
        }
      ],
      "source": [
        "dummy_input['attention_mask'].cpu().reshape(1, max_len).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKqOE96-wa4l",
        "outputId": "9ddc1fd3-276e-4cd4-8f8e-7ca818fe2587"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0]])"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 17.3 ms (started: 2024-05-18 08:11:42 +00:00)\n"
          ]
        }
      ],
      "source": [
        "batch['attention_mask'].cpu().reshape(1, max_len).numpy()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2460da69bf824304915dc26ae8e0d8ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "250374317d9a4cb08459bc2c92f35f30": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c6aee6683ad4fc9bf5ae17b23746024": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45fb4384dd914cab9ae68a322134ee09": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46567763914c4844a5676969b01442e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "481ab94682624ab981a95717c9009f28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e12f95259ed94fed822db2fb480b7393",
            "placeholder": "​",
            "style": "IPY_MODEL_250374317d9a4cb08459bc2c92f35f30",
            "value": "Creating CSV from Arrow format: 100%"
          }
        },
        "4db34daeff2540869846d682a46c0f0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_803d395d72a84c8985f9c6bc308f71d1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_46567763914c4844a5676969b01442e2",
            "value": 1
          }
        },
        "4fcc587f74ff4171b36f0e809d35cb8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_acfb1ea1b0e04cb39d4ba8d5309755b8",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aca69b5875e644ae9669ccc2d743a889",
            "value": 1
          }
        },
        "638d726d1cc6439fb80e447a6de683e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7bfef0686b654a7daf322675dfe88954": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df0b98f6de844596802f7b76d6217fc7",
            "placeholder": "​",
            "style": "IPY_MODEL_d9adaa1cd37649f5a0faa40112143926",
            "value": "Creating CSV from Arrow format: 100%"
          }
        },
        "803d395d72a84c8985f9c6bc308f71d1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ab7ab272cd3433287d5d1e8a038dfde": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aca69b5875e644ae9669ccc2d743a889": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "acfb1ea1b0e04cb39d4ba8d5309755b8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afe8f97f4eb04b6bae90f3f39d82cd8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7bfef0686b654a7daf322675dfe88954",
              "IPY_MODEL_4fcc587f74ff4171b36f0e809d35cb8c",
              "IPY_MODEL_ca879a396d1a4d5e869783a3b7ba8099"
            ],
            "layout": "IPY_MODEL_45fb4384dd914cab9ae68a322134ee09"
          }
        },
        "b257188ca29445b6a9ff90e73378c846": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ab7ab272cd3433287d5d1e8a038dfde",
            "placeholder": "​",
            "style": "IPY_MODEL_c579999eea9d4d4f8b5b5503207fab62",
            "value": " 1/1 [00:00&lt;00:00, 15.31ba/s]"
          }
        },
        "c579999eea9d4d4f8b5b5503207fab62": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c74f684c918d49f18e12d876648eeece": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_481ab94682624ab981a95717c9009f28",
              "IPY_MODEL_4db34daeff2540869846d682a46c0f0a",
              "IPY_MODEL_b257188ca29445b6a9ff90e73378c846"
            ],
            "layout": "IPY_MODEL_3c6aee6683ad4fc9bf5ae17b23746024"
          }
        },
        "ca879a396d1a4d5e869783a3b7ba8099": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2460da69bf824304915dc26ae8e0d8ab",
            "placeholder": "​",
            "style": "IPY_MODEL_638d726d1cc6439fb80e447a6de683e5",
            "value": " 1/1 [00:00&lt;00:00, 14.59ba/s]"
          }
        },
        "d9adaa1cd37649f5a0faa40112143926": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df0b98f6de844596802f7b76d6217fc7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e12f95259ed94fed822db2fb480b7393": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
